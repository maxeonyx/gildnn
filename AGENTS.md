# Repository Guidance

This repository grows one cohesive implementation that we iteratively extend. Every new capability starts as a focused, minimal copy; once it works, we merge the learnings back into the shared core so the core continually evolves into the full system. See [VISION.md](VISION.md) for the long-term direction.

## Engineering Philosophy
- Take one small, validated step at a time. Never leap.
- Keep experiments runnable on tiny models and datasets so training always completes within a few steps.
- After each change, run the relevant training loop for a handful of iterations to ensure loss/metrics behave as expected.
- Preserve previous experiments and benchmarks to compare new results against old ones.
- Favor clear, minimal data pipelines and models that can scale gradually.

Always surface test metrics alongside training onesâ€”training accuracy alone is meaningless. Every experiment must report the test benchmark it targets.

Lock every experiment to a committed configuration file (e.g., `config.json`) that records its RNG seed. If the file doesn't exist yet, create it during the run and commit it so future executions remain deterministic.

Emit a deterministic, notebook-style markdown artifact for each experiment run. Reserve clearly commented sections for human-written context (overview, hypotheses, etc.), and have the code overwrite only the autogenerated sections (configuration summary, metrics, sampled predictions, etc.).

For every new experiment, execute the deterministic workflow end-to-end: run the experiment in full mode and commit the refreshed report, run it again in test mode and commit the resulting benchmark snapshot, then rerun both modes once more to verify they leave the working tree clean.

Always ensure models compile, run, and perform at least a couple of training iterations successfully before considering a task complete.
