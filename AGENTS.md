# Repository Guidance

This repository grows one cohesive implementation that we iteratively extend. Every new capability starts as a focused, minimal copy; once it works, we merge the learnings back into the shared core so the core continually evolves into the full system. See [VISION.md](VISION.md) for the long-term direction.

## Engineering Philosophy
- Take one small, validated step at a time. Never leap.
- Keep experiments runnable on tiny models and datasets so training always completes within a few steps.
- After each change, run the relevant training loop for a handful of iterations to ensure loss/metrics behave as expected.
- Preserve previous experiments and benchmarks to compare new results against old ones.
- Favor clear, minimal data pipelines and models that can scale gradually.

Always surface test metrics alongside training onesâ€”training accuracy alone is meaningless. Every experiment must report the test benchmark it targets.

Lock every experiment to a committed configuration file (e.g., `config.json`) that records its RNG seed. If the file doesn't exist yet, create it during the run and commit it so future executions remain deterministic.

Emit a deterministic, notebook-style markdown artifact for each experiment run. Reserve clearly commented sections for human-written context (overview, hypotheses, etc.), and have the code overwrite only the autogenerated sections (configuration summary, metrics, sampled predictions, etc.).

Always ensure models compile, run, and perform at least a couple of training iterations successfully before considering a task complete.
